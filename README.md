# BERT-Question-Answering

In this project, I have trained the BERT's question-answering model from scratch.

The code was run on Google Colab using its GPU.

It took 4 hours and 15 minutes to train the entire model.

The data used can be found here: https://rajpurkar.github.io/SQuAD-explorer/

After training, I gave three different paragraphs and some questions whose answers can be found in those paragraph.

Outcome: I am highly impressed by BERT's ability to understand the context of the paragraphs and was able to answer all the questions correctly. All these questions were actually tricky.

Moreover, the test paragraphs were recently written and there is no way that they could be present in the training set.
